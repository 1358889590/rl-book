{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CliffWalking-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "import imp\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import scipy\n",
    "import gym\n",
    "\n",
    "imp.reload(logging)\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        stream=sys.stdout, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Environment\n",
    "\n",
    "import environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:15:58 [INFO] observation_space = Discrete(48)\n",
      "22:15:58 [INFO] action_space = Discrete(4)\n",
      "22:15:58 [INFO] number of states = 48, number of actions = 4\n",
      "22:15:58 [INFO] shape of map = (4, 12)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CliffWalking-v0')\n",
    "env.seed(0)\n",
    "logging.info('observation_space = %s', env.observation_space)\n",
    "logging.info('action_space = %s', env.action_space)\n",
    "logging.info('number of states = %s, number of actions = %s', env.nS, env.nA)\n",
    "logging.info('shape of map = %s', env.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play an episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_once(env, policy):\n",
    "    total_reward = 0\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        loc = np.unravel_index(state, env.shape)\n",
    "        action = np.random.choice(env.nA, p=policy[state])\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        logging.debug(\n",
    "                'state = %s, location = %s, action = %s, reward = %s',\n",
    "                state, loc, action, reward)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play an episode using optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.ones(env.shape, dtype=int)\n",
    "actions[-1, :] = 0\n",
    "actions[:, -1] = 2\n",
    "optimal_policy = np.eye(4)[actions.reshape(-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:15:58 [DEBUG] state = 36, location = (3, 0), action = 0, reward = -1\n",
      "22:15:58 [DEBUG] state = 24, location = (2, 0), action = 1, reward = -1\n",
      "22:15:58 [DEBUG] state = 25, location = (2, 1), action = 1, reward = -1\n",
      "22:15:58 [DEBUG] state = 26, location = (2, 2), action = 1, reward = -1\n",
      "22:15:58 [DEBUG] state = 27, location = (2, 3), action = 1, reward = -1\n",
      "22:15:58 [DEBUG] state = 28, location = (2, 4), action = 1, reward = -1\n",
      "22:15:58 [DEBUG] state = 29, location = (2, 5), action = 1, reward = -1\n",
      "22:15:58 [DEBUG] state = 30, location = (2, 6), action = 1, reward = -1\n",
      "22:15:58 [DEBUG] state = 31, location = (2, 7), action = 1, reward = -1\n",
      "22:15:58 [DEBUG] state = 32, location = (2, 8), action = 1, reward = -1\n",
      "22:15:58 [DEBUG] state = 33, location = (2, 9), action = 1, reward = -1\n",
      "22:15:58 [DEBUG] state = 34, location = (2, 10), action = 1, reward = -1\n",
      "22:15:58 [DEBUG] state = 35, location = (2, 11), action = 2, reward = -1\n",
      "22:15:58 [INFO] episode_reward = -13\n"
     ]
    }
   ],
   "source": [
    "episode_reward = play_once(env, optimal_policy)\n",
    "logging.info('episode_reward = %s', episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve Bellman Expectation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_bellman(env, policy, gamma=1.):\n",
    "    a, b = np.eye(env.nS), np.zeros((env.nS))\n",
    "    for state in range(env.nS - 1):\n",
    "        for action in range(env.nA):\n",
    "            pi = policy[state][action]\n",
    "            for p, next_state, reward, done in env.P[state][action]:\n",
    "                a[state, next_state] -= (pi * gamma * p)\n",
    "                b[state] += (pi * reward * p)\n",
    "    v = np.linalg.solve(a, b)\n",
    "    q = np.zeros((env.nS, env.nA))\n",
    "    for state in range(env.nS - 1):\n",
    "        for action in range(env.nA):\n",
    "            for p, next_state, reward, done in env.P[state][action]:\n",
    "                q[state][action] += ((reward + gamma * v[next_state]) * p)\n",
    "    return v, q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:15:58 [INFO] state value = [-71259.73993047 -71237.78971177 -71194.84815308 -71140.80283085\n",
      " -71061.70657408 -70938.91093031 -68108.48102394 -66319.1066515\n",
      " -64517.35282333 -61466.53107192 -57841.3923356  -49099.2438373\n",
      " -71264.9652292  -71247.53092066 -71217.52824197 -71120.56871129\n",
      " -71054.59876057 -70900.38265544 -69089.02487541 -67348.61705427\n",
      " -65077.28575315 -59299.02626943 -54477.04027156 -43408.92846114\n",
      " -71343.97588505 -71365.86170719 -71356.29488212 -71310.5189658\n",
      " -71248.19189086 -70916.43626143 -70432.29357882 -69519.49523581\n",
      " -69113.60088441 -66843.01499869 -60740.99702848 -33361.65231115\n",
      " -71417.87145991 -71463.16859934 -71447.66816434 -71480.47316828\n",
      " -71514.26925749 -71213.20010574 -71233.56329317 -71021.31099415\n",
      " -70840.1582772  -69680.13842415 -65008.34042541      0.        ]\n",
      "22:15:58 [INFO] action value = [[-7.12607399e+04 -7.12387897e+04 -7.12659652e+04 -7.12607399e+04]\n",
      " [-7.12387897e+04 -7.11958482e+04 -7.12485309e+04 -7.12607399e+04]\n",
      " [-7.11958482e+04 -7.11418028e+04 -7.12185282e+04 -7.12387897e+04]\n",
      " [-7.11418028e+04 -7.10627066e+04 -7.11215687e+04 -7.11958482e+04]\n",
      " [-7.10627066e+04 -7.09399109e+04 -7.10555988e+04 -7.11418028e+04]\n",
      " [-7.09399109e+04 -6.81094810e+04 -7.09013827e+04 -7.10627066e+04]\n",
      " [-6.81094810e+04 -6.63201067e+04 -6.90900249e+04 -7.09399109e+04]\n",
      " [-6.63201067e+04 -6.45183528e+04 -6.73496171e+04 -6.81094810e+04]\n",
      " [-6.45183528e+04 -6.14675311e+04 -6.50782858e+04 -6.63201067e+04]\n",
      " [-6.14675311e+04 -5.78423923e+04 -5.93000263e+04 -6.45183528e+04]\n",
      " [-5.78423923e+04 -4.91002438e+04 -5.44780403e+04 -6.14675311e+04]\n",
      " [-4.91002438e+04 -4.91002438e+04 -4.34099285e+04 -5.78423923e+04]\n",
      " [-7.12607399e+04 -7.12485309e+04 -7.13449759e+04 -7.12659652e+04]\n",
      " [-7.12387897e+04 -7.12185282e+04 -7.13668617e+04 -7.12659652e+04]\n",
      " [-7.11958482e+04 -7.11215687e+04 -7.13572949e+04 -7.12485309e+04]\n",
      " [-7.11418028e+04 -7.10555988e+04 -7.13115190e+04 -7.12185282e+04]\n",
      " [-7.10627066e+04 -7.09013827e+04 -7.12491919e+04 -7.11215687e+04]\n",
      " [-7.09399109e+04 -6.90900249e+04 -7.09174363e+04 -7.10555988e+04]\n",
      " [-6.81094810e+04 -6.73496171e+04 -7.04332936e+04 -7.09013827e+04]\n",
      " [-6.63201067e+04 -6.50782858e+04 -6.95204952e+04 -6.90900249e+04]\n",
      " [-6.45183528e+04 -5.93000263e+04 -6.91146009e+04 -6.73496171e+04]\n",
      " [-6.14675311e+04 -5.44780403e+04 -6.68440150e+04 -6.50782858e+04]\n",
      " [-5.78423923e+04 -4.34099285e+04 -6.07419970e+04 -5.93000263e+04]\n",
      " [-4.91002438e+04 -4.34099285e+04 -3.33626523e+04 -5.44780403e+04]\n",
      " [-7.12659652e+04 -7.13668617e+04 -7.14188715e+04 -7.13449759e+04]\n",
      " [-7.12485309e+04 -7.13572949e+04 -7.15178715e+04 -7.13449759e+04]\n",
      " [-7.12185282e+04 -7.13115190e+04 -7.15178715e+04 -7.13668617e+04]\n",
      " [-7.11215687e+04 -7.12491919e+04 -7.15178715e+04 -7.13572949e+04]\n",
      " [-7.10555988e+04 -7.09174363e+04 -7.15178715e+04 -7.13115190e+04]\n",
      " [-7.09013827e+04 -7.04332936e+04 -7.15178715e+04 -7.12491919e+04]\n",
      " [-6.90900249e+04 -6.95204952e+04 -7.15178715e+04 -7.09174363e+04]\n",
      " [-6.73496171e+04 -6.91146009e+04 -7.15178715e+04 -7.04332936e+04]\n",
      " [-6.50782858e+04 -6.68440150e+04 -7.15178715e+04 -6.95204952e+04]\n",
      " [-5.93000263e+04 -6.07419970e+04 -7.15178715e+04 -6.91146009e+04]\n",
      " [-5.44780403e+04 -3.33626523e+04 -7.15178715e+04 -6.68440150e+04]\n",
      " [-4.34099285e+04 -3.33626523e+04 -1.00000000e+00 -6.07419970e+04]\n",
      " [-7.13449759e+04 -7.15178715e+04 -7.14188715e+04 -7.14188715e+04]\n",
      " [-7.13668617e+04 -7.15178715e+04 -7.15178715e+04 -7.14188715e+04]\n",
      " [-7.13572949e+04 -7.15178715e+04 -7.15178715e+04 -7.15178715e+04]\n",
      " [-7.13115190e+04 -7.15178715e+04 -7.15178715e+04 -7.15178715e+04]\n",
      " [-7.12491919e+04 -7.15178715e+04 -7.15178715e+04 -7.15178715e+04]\n",
      " [-7.09174363e+04 -7.15178715e+04 -7.15178715e+04 -7.15178715e+04]\n",
      " [-7.04332936e+04 -7.15178715e+04 -7.15178715e+04 -7.15178715e+04]\n",
      " [-6.95204952e+04 -7.15178715e+04 -7.15178715e+04 -7.15178715e+04]\n",
      " [-6.91146009e+04 -7.15178715e+04 -7.15178715e+04 -7.15178715e+04]\n",
      " [-6.68440150e+04 -7.15178715e+04 -7.15178715e+04 -7.15178715e+04]\n",
      " [-6.07419970e+04 -1.00000000e+00 -7.15178715e+04 -7.15178715e+04]\n",
      " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "policy = np.random.uniform(size=(env.nS, env.nA))\n",
    "policy = policy / np.sum(policy, axis=1)[:, np.newaxis]\n",
    "\n",
    "state_values, action_values = evaluate_bellman(env, policy)\n",
    "logging.info('state value = %s', state_values)\n",
    "logging.info('action value = %s', action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Optimal Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:15:58 [INFO] optimal state value = [-14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3. -13. -12.\n",
      " -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2. -12. -11. -10.  -9.\n",
      "  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1. -13. -12. -11. -10.  -9.  -8.\n",
      "  -7.  -6.  -5.  -4.  -3.   0.]\n",
      "22:15:58 [INFO] optimal action value = [[ -15.  -14.  -14.  -15.]\n",
      " [ -14.  -13.  -13.  -15.]\n",
      " [ -13.  -12.  -12.  -14.]\n",
      " [ -12.  -11.  -11.  -13.]\n",
      " [ -11.  -10.  -10.  -12.]\n",
      " [ -10.   -9.   -9.  -11.]\n",
      " [  -9.   -8.   -8.  -10.]\n",
      " [  -8.   -7.   -7.   -9.]\n",
      " [  -7.   -6.   -6.   -8.]\n",
      " [  -6.   -5.   -5.   -7.]\n",
      " [  -5.   -4.   -4.   -6.]\n",
      " [  -4.   -4.   -3.   -5.]\n",
      " [ -15.  -13.  -13.  -14.]\n",
      " [ -14.  -12.  -12.  -14.]\n",
      " [ -13.  -11.  -11.  -13.]\n",
      " [ -12.  -10.  -10.  -12.]\n",
      " [ -11.   -9.   -9.  -11.]\n",
      " [ -10.   -8.   -8.  -10.]\n",
      " [  -9.   -7.   -7.   -9.]\n",
      " [  -8.   -6.   -6.   -8.]\n",
      " [  -7.   -5.   -5.   -7.]\n",
      " [  -6.   -4.   -4.   -6.]\n",
      " [  -5.   -3.   -3.   -5.]\n",
      " [  -4.   -3.   -2.   -4.]\n",
      " [ -14.  -12.  -14.  -13.]\n",
      " [ -13.  -11. -113.  -13.]\n",
      " [ -12.  -10. -113.  -12.]\n",
      " [ -11.   -9. -113.  -11.]\n",
      " [ -10.   -8. -113.  -10.]\n",
      " [  -9.   -7. -113.   -9.]\n",
      " [  -8.   -6. -113.   -8.]\n",
      " [  -7.   -5. -113.   -7.]\n",
      " [  -6.   -4. -113.   -6.]\n",
      " [  -5.   -3. -113.   -5.]\n",
      " [  -4.   -2. -113.   -4.]\n",
      " [  -3.   -2.   -1.   -3.]\n",
      " [ -13. -113.  -14.  -14.]\n",
      " [ -12. -113. -113.  -14.]\n",
      " [ -11. -113. -113. -113.]\n",
      " [ -10. -113. -113. -113.]\n",
      " [  -9. -113. -113. -113.]\n",
      " [  -8. -113. -113. -113.]\n",
      " [  -7. -113. -113. -113.]\n",
      " [  -6. -113. -113. -113.]\n",
      " [  -5. -113. -113. -113.]\n",
      " [  -4. -113. -113. -113.]\n",
      " [  -3.   -1. -113. -113.]\n",
      " [   0.    0.    0.    0.]]\n"
     ]
    }
   ],
   "source": [
    "optimal_state_values, optimal_action_values = evaluate_bellman(env, optimal_policy)\n",
    "logging.info('optimal state value = %s', optimal_state_values)\n",
    "logging.info('optimal action value = %s', optimal_action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve Bellman Optimal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_bellman(env, gamma=1.):\n",
    "    p = np.zeros((env.nS, env.nA, env.nS))\n",
    "    r = np.zeros((env.nS, env.nA))\n",
    "    for state in range(env.nS - 1):\n",
    "        for action in range(env.nA):\n",
    "            for prob, next_state, reward, done in env.P[state][action]:\n",
    "                p[state, action, next_state] += prob\n",
    "                r[state, action] += (reward * prob)\n",
    "    c = np.ones(env.nS)\n",
    "    a_ub = gamma * p.reshape(-1, env.nS) - \\\n",
    "            np.repeat(np.eye(env.nS), env.nA, axis=0)\n",
    "    b_ub = -r.reshape(-1)\n",
    "    a_eq = np.zeros((0, env.nS))\n",
    "    b_eq = np.zeros(0)\n",
    "    bounds = [(None, None),] * env.nS\n",
    "    res = scipy.optimize.linprog(c, a_ub, b_ub, bounds=bounds,\n",
    "            method='interior-point')\n",
    "    v = res.x\n",
    "    q = r + gamma * np.dot(p, v)\n",
    "    return v, q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:15:58 [INFO] optimal state value = [-1.40000000e+01 -1.30000000e+01 -1.20000000e+01 -1.10000000e+01\n",
      " -1.00000000e+01 -9.00000000e+00 -8.00000000e+00 -7.00000000e+00\n",
      " -6.00000000e+00 -5.00000000e+00 -4.00000000e+00 -3.00000000e+00\n",
      " -1.30000000e+01 -1.20000000e+01 -1.10000000e+01 -1.00000000e+01\n",
      " -9.00000000e+00 -8.00000000e+00 -7.00000000e+00 -6.00000000e+00\n",
      " -5.00000000e+00 -4.00000000e+00 -3.00000000e+00 -2.00000000e+00\n",
      " -1.20000000e+01 -1.10000000e+01 -1.00000000e+01 -9.00000000e+00\n",
      " -8.00000000e+00 -7.00000000e+00 -6.00000000e+00 -5.00000000e+00\n",
      " -4.00000000e+00 -3.00000000e+00 -2.00000000e+00 -1.00000000e+00\n",
      " -1.30000000e+01 -1.20000000e+01 -1.10000000e+01 -1.00000000e+01\n",
      " -9.00000000e+00 -8.00000000e+00 -7.00000000e+00 -6.00000000e+00\n",
      " -5.00000000e+00 -4.00000000e+00 -9.99999999e-01  1.82540720e-11]\n",
      "22:15:58 [INFO] optimal action value = [[ -14.99999999  -13.99999999  -13.99999999  -14.99999999]\n",
      " [ -13.99999999  -13.          -13.          -14.99999999]\n",
      " [ -13.          -12.          -12.          -13.99999999]\n",
      " [ -12.          -11.          -11.          -13.        ]\n",
      " [ -11.          -10.          -10.          -12.        ]\n",
      " [ -10.           -9.           -9.          -11.        ]\n",
      " [  -9.           -8.           -8.          -10.        ]\n",
      " [  -8.           -7.           -7.           -9.        ]\n",
      " [  -7.           -6.           -6.           -8.        ]\n",
      " [  -6.           -5.           -5.           -7.        ]\n",
      " [  -5.           -4.           -4.           -6.        ]\n",
      " [  -4.           -4.           -3.           -5.        ]\n",
      " [ -14.99999999  -13.          -13.          -13.99999999]\n",
      " [ -13.99999999  -12.          -12.          -13.99999999]\n",
      " [ -13.          -11.          -11.          -13.        ]\n",
      " [ -12.          -10.          -10.          -12.        ]\n",
      " [ -11.           -9.           -9.          -11.        ]\n",
      " [ -10.           -8.           -8.          -10.        ]\n",
      " [  -9.           -7.           -7.           -9.        ]\n",
      " [  -8.           -6.           -6.           -8.        ]\n",
      " [  -7.           -5.           -5.           -7.        ]\n",
      " [  -6.           -4.           -4.           -6.        ]\n",
      " [  -5.           -3.           -3.           -5.        ]\n",
      " [  -4.           -3.           -2.           -4.        ]\n",
      " [ -13.99999999  -12.          -14.          -13.        ]\n",
      " [ -13.          -11.         -113.          -13.        ]\n",
      " [ -12.          -10.         -113.          -12.        ]\n",
      " [ -11.           -9.         -113.          -11.        ]\n",
      " [ -10.           -8.         -113.          -10.        ]\n",
      " [  -9.           -7.         -113.           -9.        ]\n",
      " [  -8.           -6.         -113.           -8.        ]\n",
      " [  -7.           -5.         -113.           -7.        ]\n",
      " [  -6.           -4.         -113.           -6.        ]\n",
      " [  -5.           -3.         -113.           -5.        ]\n",
      " [  -4.           -2.         -113.           -4.        ]\n",
      " [  -3.           -2.           -1.           -3.        ]\n",
      " [ -13.         -113.          -14.          -14.        ]\n",
      " [ -12.         -113.         -113.          -14.        ]\n",
      " [ -11.         -113.         -113.         -113.        ]\n",
      " [ -10.         -113.         -113.         -113.        ]\n",
      " [  -9.         -113.         -113.         -113.        ]\n",
      " [  -8.         -113.         -113.         -113.        ]\n",
      " [  -7.         -113.         -113.         -113.        ]\n",
      " [  -6.         -113.         -113.         -113.        ]\n",
      " [  -5.         -113.         -113.         -113.        ]\n",
      " [  -4.         -113.         -113.         -113.        ]\n",
      " [  -3.           -1.         -113.         -113.        ]\n",
      " [   0.            0.            0.            0.        ]]\n"
     ]
    }
   ],
   "source": [
    "optimal_state_values, optimal_action_values = optimal_bellman(env)\n",
    "logging.info('optimal state value = %s', optimal_state_values)\n",
    "logging.info('optimal action value = %s', optimal_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:15:58 [INFO] optimal policy = [2 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 2 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "optimal_actions = optimal_action_values.argmax(axis=1)\n",
    "logging.info('optimal policy = %s', optimal_actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
