{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jA6jBcLwC580"
   },
   "source": [
    "# Use DQN to Play MoutainCar-v0\n",
    "\n",
    "PyTorch version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QPrYUlr7C582"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "import imp\n",
    "import itertools\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import pandas as pd\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "torch.manual_seed(0)\n",
    "\n",
    "imp.reload(logging)\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "        format='%(asctime)s [%(levelname)s] %(message)s',\n",
    "        stream=sys.stdout, datefmt='%H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "colab_type": "code",
    "id": "lWyjPiyHC588",
    "outputId": "2bc6a5e0-9ed2-4faa-c9ea-9a0f504c127a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:50:18 [INFO] env: <MountainCarEnv<MountainCar-v0>>\n",
      "22:50:18 [INFO] action_space: Discrete(3)\n",
      "22:50:18 [INFO] observation_space: Box(-1.2000000476837158, 0.6000000238418579, (2,), float32)\n",
      "22:50:18 [INFO] reward_range: (-inf, inf)\n",
      "22:50:18 [INFO] metadata: {'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 30}\n",
      "22:50:18 [INFO] _max_episode_steps: 200\n",
      "22:50:18 [INFO] _elapsed_steps: None\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(0)\n",
    "for key in vars(env):\n",
    "    logging.info('%s: %s', key, vars(env)[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TsOqN8VJC592"
   },
   "outputs": [],
   "source": [
    "class DQNReplayer:\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = pd.DataFrame(index=range(capacity),\n",
    "                columns=['state', 'action', 'reward', 'next_state', 'done'])\n",
    "        self.i = 0\n",
    "        self.count = 0\n",
    "        self.capacity = capacity\n",
    "\n",
    "    def store(self, *args):\n",
    "        self.memory.loc[self.i] = args\n",
    "        self.i = (self.i + 1) % self.capacity\n",
    "        self.count = min(self.count + 1, self.capacity)\n",
    "\n",
    "    def sample(self, size):\n",
    "        indices = np.random.choice(self.count, size=size)\n",
    "        return (np.stack(self.memory.loc[indices, field]) for field in\n",
    "                self.memory.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZAMDYhvaEPnQ"
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "        self.action_n = env.action_space.n\n",
    "        self.gamma = 0.99\n",
    "\n",
    "        self.replayer = DQNReplayer(10000)\n",
    "\n",
    "        self.evaluate_net = self.build_net(\n",
    "                input_size=env.observation_space.shape[0],\n",
    "                hidden_sizes=[64, 64], output_size=self.action_n)\n",
    "        self.optimizer = optim.Adam(self.evaluate_net.parameters(), lr=0.001)\n",
    "        self.loss = nn.MSELoss()\n",
    "\n",
    "    def build_net(self, input_size, hidden_sizes, output_size):\n",
    "        layers = []\n",
    "        for input_size, output_size in zip(\n",
    "                [input_size,] + hidden_sizes, hidden_sizes + [output_size,]):\n",
    "            layers.append(nn.Linear(input_size, output_size))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers = layers[:-1]\n",
    "        model = nn.Sequential(*layers)\n",
    "        return model\n",
    "\n",
    "    def reset(self, mode=None):\n",
    "        self.mode = mode\n",
    "        if self.mode == 'train':\n",
    "            self.trajectory = []\n",
    "            self.target_net = copy.deepcopy(self.evaluate_net)\n",
    "\n",
    "    def step(self, observation, reward, done):\n",
    "        if self.mode == 'train' and np.random.rand() < 0.001:\n",
    "            # epsilon-greedy policy in train mode\n",
    "            action = np.random.randint(self.action_n)\n",
    "        else:\n",
    "            state_tensor = torch.as_tensor(observation,\n",
    "                    dtype=torch.float).squeeze(0)\n",
    "            q_tensor = self.evaluate_net(state_tensor)\n",
    "            action_tensor = torch.argmax(q_tensor)\n",
    "            action = action_tensor.item()\n",
    "        if self.mode == 'train':\n",
    "            self.trajectory += [observation, reward, done, action]\n",
    "            if len(self.trajectory) >= 8:\n",
    "                state, _, _, action, next_state, reward, done, _ = \\\n",
    "                        self.trajectory[-8:]\n",
    "                self.replayer.store(state, action, reward, next_state, done)\n",
    "            if self.replayer.count >= self.replayer.capacity * 0.95:\n",
    "                    # skip first few episodes for speed\n",
    "                self.learn()\n",
    "        return action\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        # replay\n",
    "        states, actions, rewards, next_states, dones = \\\n",
    "                self.replayer.sample(1024) # replay transitions\n",
    "        state_tensor = torch.as_tensor(states, dtype=torch.float)\n",
    "        action_tensor = torch.as_tensor(actions, dtype=torch.long)\n",
    "        reward_tensor = torch.as_tensor(rewards, dtype=torch.float)\n",
    "        next_state_tensor = torch.as_tensor(next_states, dtype=torch.float)\n",
    "        done_tensor = torch.as_tensor(dones, dtype=torch.float)\n",
    "\n",
    "        # train\n",
    "        next_q_tensor = self.target_net(next_state_tensor)\n",
    "        next_max_q_tensor, _ = next_q_tensor.max(axis=-1)\n",
    "        target_tensor = reward_tensor + self.gamma * (1. - done_tensor) * next_max_q_tensor\n",
    "        pred_tensor = self.evaluate_net(state_tensor)\n",
    "        q_tensor = pred_tensor.gather(1, action_tensor.unsqueeze(1)).squeeze(1)\n",
    "        loss_tensor = self.loss(target_tensor, q_tensor)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss_tensor.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "agent = DQNAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDcTjATVEcRQ",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:50:18 [INFO] ==== train ====\n",
      "22:50:19 [DEBUG] train episode 0: reward = -200.00, steps = 200\n",
      "22:50:19 [DEBUG] train episode 1: reward = -200.00, steps = 200\n",
      "22:50:19 [DEBUG] train episode 2: reward = -200.00, steps = 200\n",
      "22:50:19 [DEBUG] train episode 3: reward = -200.00, steps = 200\n",
      "22:50:19 [DEBUG] train episode 4: reward = -200.00, steps = 200\n",
      "22:50:20 [DEBUG] train episode 5: reward = -200.00, steps = 200\n",
      "22:50:20 [DEBUG] train episode 6: reward = -200.00, steps = 200\n",
      "22:50:20 [DEBUG] train episode 7: reward = -200.00, steps = 200\n",
      "22:50:20 [DEBUG] train episode 8: reward = -200.00, steps = 200\n",
      "22:50:20 [DEBUG] train episode 9: reward = -200.00, steps = 200\n",
      "22:50:20 [DEBUG] train episode 10: reward = -200.00, steps = 200\n",
      "22:50:21 [DEBUG] train episode 11: reward = -200.00, steps = 200\n",
      "22:50:21 [DEBUG] train episode 12: reward = -200.00, steps = 200\n",
      "22:50:21 [DEBUG] train episode 13: reward = -200.00, steps = 200\n",
      "22:50:21 [DEBUG] train episode 14: reward = -200.00, steps = 200\n",
      "22:50:21 [DEBUG] train episode 15: reward = -200.00, steps = 200\n",
      "22:50:22 [DEBUG] train episode 16: reward = -200.00, steps = 200\n",
      "22:50:22 [DEBUG] train episode 17: reward = -200.00, steps = 200\n",
      "22:50:22 [DEBUG] train episode 18: reward = -200.00, steps = 200\n",
      "22:50:22 [DEBUG] train episode 19: reward = -200.00, steps = 200\n",
      "22:50:22 [DEBUG] train episode 20: reward = -200.00, steps = 200\n",
      "22:50:22 [DEBUG] train episode 21: reward = -200.00, steps = 200\n",
      "22:50:23 [DEBUG] train episode 22: reward = -200.00, steps = 200\n",
      "22:50:23 [DEBUG] train episode 23: reward = -200.00, steps = 200\n",
      "22:50:23 [DEBUG] train episode 24: reward = -200.00, steps = 200\n",
      "22:50:23 [DEBUG] train episode 25: reward = -200.00, steps = 200\n",
      "22:50:23 [DEBUG] train episode 26: reward = -200.00, steps = 200\n",
      "22:50:23 [DEBUG] train episode 27: reward = -200.00, steps = 200\n",
      "22:50:24 [DEBUG] train episode 28: reward = -200.00, steps = 200\n",
      "22:50:24 [DEBUG] train episode 29: reward = -200.00, steps = 200\n",
      "22:50:24 [DEBUG] train episode 30: reward = -200.00, steps = 200\n",
      "22:50:24 [DEBUG] train episode 31: reward = -200.00, steps = 200\n",
      "22:50:24 [DEBUG] train episode 32: reward = -200.00, steps = 200\n",
      "22:50:25 [DEBUG] train episode 33: reward = -200.00, steps = 200\n",
      "22:50:25 [DEBUG] train episode 34: reward = -200.00, steps = 200\n",
      "22:50:25 [DEBUG] train episode 35: reward = -200.00, steps = 200\n",
      "22:50:25 [DEBUG] train episode 36: reward = -200.00, steps = 200\n",
      "22:50:25 [DEBUG] train episode 37: reward = -200.00, steps = 200\n",
      "22:50:25 [DEBUG] train episode 38: reward = -200.00, steps = 200\n",
      "22:50:26 [DEBUG] train episode 39: reward = -200.00, steps = 200\n",
      "22:50:26 [DEBUG] train episode 40: reward = -200.00, steps = 200\n",
      "22:50:26 [DEBUG] train episode 41: reward = -200.00, steps = 200\n",
      "22:50:26 [DEBUG] train episode 42: reward = -200.00, steps = 200\n",
      "22:50:26 [DEBUG] train episode 43: reward = -200.00, steps = 200\n",
      "22:50:27 [DEBUG] train episode 44: reward = -200.00, steps = 200\n",
      "22:50:27 [DEBUG] train episode 45: reward = -200.00, steps = 200\n",
      "22:50:27 [DEBUG] train episode 46: reward = -200.00, steps = 200\n",
      "22:50:49 [DEBUG] train episode 47: reward = -200.00, steps = 200\n",
      "22:52:12 [DEBUG] train episode 48: reward = -200.00, steps = 200\n",
      "22:53:25 [DEBUG] train episode 49: reward = -200.00, steps = 200\n",
      "22:55:29 [DEBUG] train episode 50: reward = -200.00, steps = 200\n",
      "22:58:47 [DEBUG] train episode 51: reward = -200.00, steps = 200\n",
      "23:00:20 [DEBUG] train episode 52: reward = -200.00, steps = 200\n",
      "23:01:34 [DEBUG] train episode 53: reward = -200.00, steps = 200\n",
      "23:02:42 [DEBUG] train episode 54: reward = -200.00, steps = 200\n",
      "23:03:50 [DEBUG] train episode 55: reward = -200.00, steps = 200\n",
      "23:04:55 [DEBUG] train episode 56: reward = -200.00, steps = 200\n",
      "23:06:00 [DEBUG] train episode 57: reward = -200.00, steps = 200\n",
      "23:07:03 [DEBUG] train episode 58: reward = -200.00, steps = 200\n",
      "23:08:06 [DEBUG] train episode 59: reward = -200.00, steps = 200\n",
      "23:09:09 [DEBUG] train episode 60: reward = -200.00, steps = 200\n",
      "23:10:33 [DEBUG] train episode 61: reward = -200.00, steps = 200\n",
      "23:11:56 [DEBUG] train episode 62: reward = -200.00, steps = 200\n",
      "23:13:02 [DEBUG] train episode 63: reward = -200.00, steps = 200\n",
      "23:14:06 [DEBUG] train episode 64: reward = -200.00, steps = 200\n",
      "23:15:12 [DEBUG] train episode 65: reward = -200.00, steps = 200\n",
      "23:16:16 [DEBUG] train episode 66: reward = -200.00, steps = 200\n",
      "23:17:22 [DEBUG] train episode 67: reward = -200.00, steps = 200\n",
      "23:18:26 [DEBUG] train episode 68: reward = -200.00, steps = 200\n",
      "23:19:36 [DEBUG] train episode 69: reward = -200.00, steps = 200\n",
      "23:20:43 [DEBUG] train episode 70: reward = -200.00, steps = 200\n",
      "23:21:47 [DEBUG] train episode 71: reward = -200.00, steps = 200\n",
      "23:23:00 [DEBUG] train episode 72: reward = -200.00, steps = 200\n",
      "23:24:11 [DEBUG] train episode 73: reward = -200.00, steps = 200\n",
      "23:25:18 [DEBUG] train episode 74: reward = -200.00, steps = 200\n",
      "23:26:28 [DEBUG] train episode 75: reward = -200.00, steps = 200\n",
      "23:27:41 [DEBUG] train episode 76: reward = -200.00, steps = 200\n",
      "23:28:55 [DEBUG] train episode 77: reward = -200.00, steps = 200\n",
      "23:30:07 [DEBUG] train episode 78: reward = -200.00, steps = 200\n",
      "23:31:06 [DEBUG] train episode 79: reward = -200.00, steps = 200\n",
      "23:31:53 [DEBUG] train episode 80: reward = -200.00, steps = 200\n",
      "23:32:31 [DEBUG] train episode 81: reward = -200.00, steps = 200\n",
      "23:33:18 [DEBUG] train episode 82: reward = -200.00, steps = 200\n",
      "23:34:05 [DEBUG] train episode 83: reward = -200.00, steps = 200\n",
      "23:34:28 [DEBUG] train episode 84: reward = -200.00, steps = 200\n",
      "23:35:00 [DEBUG] train episode 85: reward = -200.00, steps = 200\n",
      "23:35:48 [DEBUG] train episode 86: reward = -200.00, steps = 200\n",
      "23:36:33 [DEBUG] train episode 87: reward = -200.00, steps = 200\n",
      "23:37:18 [DEBUG] train episode 88: reward = -200.00, steps = 200\n",
      "23:38:02 [DEBUG] train episode 89: reward = -200.00, steps = 200\n",
      "23:38:44 [DEBUG] train episode 90: reward = -200.00, steps = 200\n",
      "23:39:30 [DEBUG] train episode 91: reward = -200.00, steps = 200\n",
      "23:40:17 [DEBUG] train episode 92: reward = -200.00, steps = 200\n",
      "23:41:05 [DEBUG] train episode 93: reward = -200.00, steps = 200\n",
      "23:41:49 [DEBUG] train episode 94: reward = -200.00, steps = 200\n",
      "23:42:32 [DEBUG] train episode 95: reward = -200.00, steps = 200\n",
      "23:43:16 [DEBUG] train episode 96: reward = -200.00, steps = 200\n",
      "23:44:09 [DEBUG] train episode 97: reward = -200.00, steps = 200\n",
      "23:44:43 [DEBUG] train episode 98: reward = -200.00, steps = 200\n",
      "23:45:39 [DEBUG] train episode 99: reward = -200.00, steps = 200\n",
      "23:46:18 [DEBUG] train episode 100: reward = -200.00, steps = 200\n",
      "23:47:12 [DEBUG] train episode 101: reward = -200.00, steps = 200\n",
      "23:47:49 [DEBUG] train episode 102: reward = -200.00, steps = 200\n",
      "23:48:42 [DEBUG] train episode 103: reward = -200.00, steps = 200\n"
     ]
    }
   ],
   "source": [
    "def play_episode(env, agent, max_episode_steps=None, mode=None, render=False):\n",
    "    observation, reward, done = env.reset(), 0., False\n",
    "    agent.reset(mode=mode)\n",
    "    episode_reward, elapsed_steps = 0., 0\n",
    "    while True:\n",
    "        action = agent.step(observation, reward, done)\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:\n",
    "            break\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        elapsed_steps += 1\n",
    "        if max_episode_steps and elapsed_steps >= max_episode_steps:\n",
    "            break\n",
    "    agent.close()\n",
    "    return episode_reward, elapsed_steps\n",
    "\n",
    "\n",
    "logging.info('==== train ====')\n",
    "episode_rewards = []\n",
    "for episode in itertools.count():\n",
    "    episode_reward, elapsed_steps = play_episode(env.unwrapped, agent,\n",
    "            max_episode_steps=env._max_episode_steps, mode='train')\n",
    "    episode_rewards.append(episode_reward)\n",
    "    logging.debug('train episode %d: reward = %.2f, steps = %d',\n",
    "            episode, episode_reward, elapsed_steps)\n",
    "    if np.mean(episode_rewards[-10:]) > -110:\n",
    "        break\n",
    "plt.plot(episode_rewards)\n",
    "\n",
    "\n",
    "logging.info('==== test ====')\n",
    "episode_rewards = []\n",
    "for episode in range(100):\n",
    "    episode_reward, elapsed_steps = play_episode(env, agent)\n",
    "    episode_rewards.append(episode_reward)\n",
    "    logging.debug('test episode %d: reward = %.2f, steps = %d',\n",
    "            episode, episode_reward, elapsed_steps)\n",
    "logging.info('average episode reward = %.2f ± %.2f',\n",
    "        np.mean(episode_rewards), np.std(episode_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MountainCar_v0_201901302220.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
